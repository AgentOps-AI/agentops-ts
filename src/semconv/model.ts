
export const GEN_AI_REQUEST_MODEL = 'gen_ai.request.model';
export const GEN_AI_REQUEST_MAX_TOKENS = 'gen_ai.request.max_tokens';
export const GEN_AI_REQUEST_TEMPERATURE = 'gen_ai.request.temperature';
export const GEN_AI_REQUEST_TOP_P = 'gen_ai.request.top_p';
export const GEN_AI_REQUEST_FREQUENCY_PENALTY = 'gen_ai.request.frequency_penalty';
export const GEN_AI_REQUEST_PRESENCE_PENALTY = 'gen_ai.request.presence_penalty';
export const GEN_AI_REQUEST_STOP_SEQUENCES = 'gen_ai.request.stop_sequences';

export const GEN_AI_RESPONSE_MODEL = 'gen_ai.response.model';
export const GEN_AI_RESPONSE_FINISH_REASONS = 'gen_ai.response.finish_reasons';

export const GEN_AI_USAGE_INPUT_TOKENS = 'gen_ai.usage.prompt_tokens';
export const GEN_AI_USAGE_OUTPUT_TOKENS = 'gen_ai.usage.completion_tokens';
export const GEN_AI_USAGE_TOTAL_TOKENS = 'gen_ai.usage.total_tokens';
// TODO cache and reasoning tokens